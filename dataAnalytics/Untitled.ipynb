{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# import numpy as np\n",
    "# file_names = []\n",
    "# for fn in os.listdir('../../tmp/historyStorage/'):\n",
    "#     file_names.append(fn)\n",
    "# type(file_names[0])\n",
    "# # file_names\n",
    "\n",
    "# %load processJSONData.py\n",
    "# historyStorage_1477123900862362.json\n",
    "file_searches = []\n",
    "import json\n",
    "import seaborn as sns\n",
    "# for i in file_names:\n",
    "with open(url) as data_file:    \n",
    "    file_searches.append(json.loads(data_file.read()))\n",
    "\n",
    "\n",
    "\n",
    "import itertools\n",
    "searches = list(itertools.chain.from_iterable(file_searches))\n",
    "# searches\n",
    "\n",
    "for search in searches:\n",
    "    url = search['url']\n",
    "    lastVisitTime = search['lastVisitTime']\n",
    "    title = search['title']\n",
    "    typedCount = search['typedCount']\n",
    "    visitCount = search['visitCount']\n",
    "\n",
    "\n",
    "dimension = len(searches)\n",
    "url = []\n",
    "lastVisitTime = []\n",
    "title = []\n",
    "typedCount = []\n",
    "visitCount = []\n",
    "\n",
    "for i in range(dimension):\n",
    "    url.append(str(searches[i]['url']))\n",
    "    lastVisitTime.append(searches[i]['lastVisitTime'])\n",
    "    title.append(searches[i]['title'])\n",
    "    typedCount.append(searches[i]['typedCount'])\n",
    "    visitCount.append(searches[i]['visitCount'])\n",
    "\n",
    "import re\n",
    "def getDomain(url):\n",
    "    #requires 'http://' or 'https://'\n",
    "    #pat = r'(https?):\\/\\/(\\w+\\.)*(?P<domain>\\w+)\\.(\\w+)(\\/.*)?'\n",
    "    #'http://' or 'https://' is optional\n",
    "    pat = r'((https?):\\/\\/)?(\\w+\\.)*(?P<domain>\\w+)\\.(\\w+)(\\/.*)?'\n",
    "    m = re.match(pat, url)\n",
    "    if m:\n",
    "        domain = m.group('domain')\n",
    "        return domain\n",
    "\n",
    "domains = []\n",
    "for i in url:\n",
    "    domains.append(getDomain(i))\n",
    "\n",
    "import numpy as np\n",
    "lastVisitTime = np.sort(lastVisitTime)\n",
    "# time.time()\n",
    "\n",
    "import time\n",
    "visitTimes = []\n",
    "for i in lastVisitTime:\n",
    "    visitTimes.append(time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime(i/1000)))\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(data = {'domains': domains, \n",
    "                     'visitTimes': visitTimes,\n",
    "                     'title': title, \n",
    "                     'typedCount': typedCount, \n",
    "                     'visitCount': visitCount})\n",
    "\n",
    "# df = df.reindex(index=df.index[::-1], copy=True)\n",
    "# df.index = range(dimension)\n",
    "# df\n",
    "\n",
    "df_counts = df.groupby(df.domains).count()\n",
    "# df_counts\n",
    "\n",
    "df_reasonable = df_counts[df_counts['title'] >= 10]\n",
    "\n",
    "import matplotlib\n",
    "# %matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=[10,5])\n",
    "domain_hist = df_reasonable.title.plot(kind=\"bar\")\n",
    "plt.ylabel('Frequency', ); plt.xlabel('Domains')\n",
    "plt.title('All searches (10 queries minimum)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.savefig('domain_frequencies_local.png', bbox_inches='tight')\n",
    "\n",
    "# df[df.domains == 'google']\n",
    "\n",
    "stackoverflow_query = df[df.domains == 'stackoverflow']\n",
    "\n",
    "google_query = df[df.domains == 'google']\n",
    "\n",
    "non_google = df[(df.domains != 'google') & (df.domains != 'stackoverflow')]\n",
    "\n",
    "# non_google\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words = 'english', max_df= .8)\n",
    "# min_df = 0.1 means that the term must be in at least 10% of the documents\n",
    "X = vectorizer.fit_transform(non_google.title)\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "k = 4 # Define the number of clusters in which we want to partion our data\n",
    "# Define the proper notion of distance to deal with documents\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "dist = 1 - cosine_similarity(X)\n",
    "# Run the algorithm kmeans\n",
    "model = KMeans(n_clusters = k)\n",
    "model.fit(X);\n",
    "\n",
    "no_words = 2 # Number of words to print per cluster\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1] # Sort cluster centers by proximity to centroid\n",
    "terms = vectorizer.get_feature_names()\n",
    "labels = model.labels_ # Get labels assigned to each data\n",
    "\n",
    "print(\"Top non google/S.O terms per cluster:\\n\")\n",
    "for i in range(k):\n",
    "\n",
    "#     print(\"Cluster %d topics:\" % i)\n",
    "#     for title in non_google[\"title\"][labels == i]:\n",
    "#         print(' %s,' % title)\n",
    "#     print #add a whitespace\n",
    "\n",
    "    print(\"Cluster %d words:\" % i,) \n",
    "    for ind in order_centroids[i, :no_words]:\n",
    "        print (' %s' % terms[ind],),\n",
    "    print\n",
    "    print\n",
    "\n",
    "# stackoverflow_query.title\n",
    "\n",
    "np.flipud(lastVisitTime) \n",
    "fixed_int_times = []\n",
    "for i in lastVisitTime:\n",
    "    fixed_int_times.append((i - np.mean(lastVisitTime))/np.std(lastVisitTime))\n",
    "\n",
    "import datetime\n",
    "midnight = datetime.datetime(2016, 10, 23, 0, 0)\n",
    "night_start = (midnight-datetime.datetime(1970,1,1)).total_seconds()\n",
    "# night_start\n",
    "\n",
    "timezone_diff = datetime.datetime(2016, 10, 22, 8, 0)\n",
    "true_night_start = (timezone_diff-datetime.datetime(1970,1,1)).total_seconds()\n",
    "# true_night_start\n",
    "\n",
    "noon = datetime.datetime(2016, 10, 22, 20, 0)\n",
    "midday_start = (noon-datetime.datetime(1970,1,1)).total_seconds()\n",
    "# midday_start\n",
    "\n",
    "google_search = df[df.domains == 'google']\n",
    "google_search_times = []\n",
    "for i in google_search.index:\n",
    "     google_search_times.append(lastVisitTime[i])\n",
    "\n",
    "df_all_times = []\n",
    "for i in df.index:\n",
    "     df_all_times.append(lastVisitTime[i])\n",
    "\n",
    "stackoverflow_search = df[df.domains == 'stackoverflow']\n",
    "stackoverflow_search_times = []\n",
    "for i in stackoverflow_search.index:\n",
    "    stackoverflow_search_times.append(lastVisitTime[i])\n",
    "\n",
    "both_search = df[(df.domains == 'stackoverflow') | (df.domains == 'google')]\n",
    "both_search_times = []\n",
    "for i in both_search.index:\n",
    "    both_search_times.append(lastVisitTime[i])\n",
    "\n",
    "# plt.figure(figsize=[15,10])\n",
    "plt.plot(df_all_times,range(len(df_all_times)), c = 'blue', label= 'all sites')\n",
    "plt.plot(stackoverflow_search_times,range(len(stackoverflow_search_times)), c = 'red', label = 'stack overflow')\n",
    "plt.plot(google_search_times,range(len(google_search_times)), c = 'green', label = 'google')\n",
    "plt.plot(both_search_times,range(len(both_search_times)), c = 'purple', label = 'both')\n",
    "plt.axvline(true_night_start*1000, color = 'black')\n",
    "plt.text(true_night_start*1000 + 850000, len(df)-50, s = 'Midnight, 10/23', color = 'black')\n",
    "plt.legend(loc = 'upper left')\n",
    "\n",
    "plt.axvline(midday_start*1000, color = 'black')\n",
    "plt.text(midday_start*1000 + 850000, len(df)-50, s = 'Noon, 10/23', color = 'black')\n",
    "\n",
    "plt.xlabel('Time'); plt.ylabel('Cumulative Frequency')\n",
    "\n",
    "plt.title('Cumulative Search Density Over Time')\n",
    "plt.tight_layout()\n",
    "plt.autoscale(tight=True)\n",
    "plt.show()\n",
    "plt.savefig('cumulative_densities_local.png', bbox_inches='tight')\n",
    "\n",
    "python_df = df[df['title'].str.contains(\"python|Python\")==True]\n",
    "javascript_df = df[df['title'].str.contains(\"javascript|Javascript\")==True]\n",
    "API_df = df[df['title'].str.contains(\"API\")]\n",
    "swift_df = df[df['title'].str.contains(\"swift|Swift\")==True]\n",
    "aws_df = df[df['title'].str.contains(\"aws|AWS\")==True]\n",
    "how_to_df = df[df['title'].str.contains(\"how to|How to\")==True]\n",
    "html_df = df[df['title'].str.contains(\"html|HTML|Html\")==True]\n",
    "angularjs_df = df[df['title'].str.contains(\"AngularJS|angularjs\")==True]\n",
    "microsoft_df = df[df['title'].str.contains(\"microsoft|Microsoft\")==True]\n",
    "apple_df = df[df['title'].str.contains(\"Apple|apple|mac|OS\")==True]\n",
    "\n",
    "javascript_hits = len(javascript_df)\n",
    "python_hits = len(python_df)\n",
    "API_hits = len(API_df)\n",
    "swift_hits = len(swift_df)\n",
    "AWS_hits = len(aws_df)\n",
    "how_to_hits = len(how_to_df)\n",
    "html_hits = len(html_df)\n",
    "angularjs_hits = len(angularjs_df)\n",
    "microsoft_hits = len(microsoft_df)\n",
    "apple_hits = len(apple_df)\n",
    "buzzwords = [javascript_hits, python_hits, API_hits, swift_hits, AWS_hits, how_to_hits, html_hits,\n",
    "            angularjs_hits, microsoft_hits, apple_hits]\n",
    "\n",
    "s = pd.Series(\n",
    "    buzzwords,\n",
    "    index = ['JS', 'Python', 'API', 'Swift', 'AWS', 'How to...', 'HTML', \n",
    "            'AngularJS', 'Microsoft', 'Apple']\n",
    ")\n",
    "x = np.arange(len(buzzwords))\n",
    "\n",
    "#Set descriptions:\n",
    "plt.title(\"Frequency of searches\")\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Hackathon Buzzwords')\n",
    "\n",
    "#Set tick colors:\n",
    "ax = plt.gca()\n",
    "ax.tick_params(axis='x')\n",
    "ax.tick_params(axis='y')\n",
    "\n",
    "#Plot the data:\n",
    "cmap = plt.get_cmap('gist_rainbow')\n",
    "my_colors = [cmap(i) for i in np.linspace(0, 1, 10)]\n",
    "\n",
    "s.plot( \n",
    "    kind='bar', \n",
    "    color=my_colors,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.savefig('buzzwords_local.png', bbox_inches='tight')\n",
    "\n",
    "def CBR_ratio(total_searches, google_searches, stackoverflow_searches):\n",
    "    google_ratio = len(google_searches) / (len(total_searches) + 0.0)\n",
    "    stackoverflow_ratio = len(stackoverflow_searches) / (len(total_searches) + 0.0)\n",
    "\n",
    "    return google_ratio, stackoverflow_ratio\n",
    "\n",
    "# CBR_ratio(df_all_times, google_search_times, stackoverflow_search_times)\n",
    "\n",
    "df_times = np.array(df_all_times) /1000\n",
    "\n",
    "# df_times\n",
    "\n",
    "noon = datetime.datetime(2016, 10, 22, 20, 0)\n",
    "one = datetime.datetime(2016, 10, 22, 21, 0)\n",
    "hour_time = (one-noon).total_seconds()\n",
    "# hour_time\n",
    "total_hours = int((df_times[-1] - df_times[0])/hour_time)\n",
    "# total_hours\n",
    "\n",
    "# df\n",
    "\n",
    "time_stamps = ['2016-10-22 01:00:00',\n",
    "               '2016-10-22 02:00:00',\n",
    "               '2016-10-22 03:00:00',\n",
    "               '2016-10-22 04:00:00',\n",
    "               '2016-10-22 05:00:00',\n",
    "               '2016-10-22 06:00:00',\n",
    "               '2016-10-22 07:00:00',\n",
    "               '2016-10-22 08:00:00',\n",
    "               '2016-10-22 09:00:00',\n",
    "               '2016-10-22 10:00:00',\n",
    "               '2016-10-22 11:00:00',\n",
    "               '2016-10-22 12:00:00',\n",
    "               '2016-10-22 13:00:00',\n",
    "               '2016-10-22 14:00:00',\n",
    "               '2016-10-22 15:00:00',\n",
    "               '2016-10-22 16:00:00',\n",
    "               '2016-10-22 17:00:00',\n",
    "               '2016-10-22 18:00:00',\n",
    "               '2016-10-22 19:00:00',\n",
    "               '2016-10-22 20:00:00',\n",
    "               '2016-10-22 21:00:00',\n",
    "               '2016-10-22 22:00:00',\n",
    "               '2016-10-22 23:00:00',\n",
    "               '2016-10-23 00:00:00',\n",
    "               '2016-10-23 01:00:00',\n",
    "#                '2016-10-23 02:00:00',\n",
    "#                '2016-10-23 03:00:00',\n",
    "#                '2016-10-23 04:00:00',\n",
    "#                '2016-10-23 05:00:00',\n",
    "#                '2016-10-23 06:00:00',\n",
    "#                '2016-10-23 07:00:00',\n",
    "#                '2016-10-23 08:00:00',\n",
    "#                '2016-10-23 09:00:00',\n",
    "#                '2016-10-23 10:00:00',\n",
    "#                '2016-10-23 11:00:00',\n",
    "#                '2016-10-23 12:00:00',\n",
    "#                '2016-10-23 13:00:00',\n",
    "#                '2016-10-23 14:00:00',\n",
    "#                '2016-10-23 15:00:00',\n",
    "#                '2016-10-23 16:00:00',\n",
    "#                '2016-10-23 17:00:00',\n",
    "#                '2016-10-23 18:00:00',\n",
    "#                '2016-10-23 19:00:00',\n",
    "#                '2016-10-23 20:00:00',\n",
    "#                '2016-10-23 21:00:00',\n",
    "#                '2016-10-23 22:00:00',\n",
    "#                '2016-10-23 23:00:00',\n",
    "#                '2016-10-24 00:00:00',\n",
    "#                '2016-10-24 01:00:00',\n",
    "#                '2016-10-24 02:00:00',\n",
    "#                '2016-10-24 03:00:00',\n",
    "#                '2016-10-24 04:00:00',\n",
    "#                '2016-10-24 05:00:00',\n",
    "#                '2016-10-24 06:00:00',\n",
    "#                '2016-10-24 07:00:00',\n",
    "#                '2016-10-24 08:00:00',\n",
    "#                '2016-10-24 09:00:00',\n",
    "              ]\n",
    "\n",
    "time_index = np.empty([len(time_stamps)])\n",
    "for i in range(len(time_stamps)):\n",
    "    if i == 0:\n",
    "        df_split = df[df.visitTimes < time_stamps[i]]\n",
    "    else:\n",
    "        df_split = df[(df.visitTimes < time_stamps[i]) & (df.visitTimes > time_stamps[i-1])]\n",
    "    time_index[i] = len(df_split)\n",
    "\n",
    "# time_index\n",
    "\n",
    "plt.plot(range(8, len(time_stamps) + 8), time_index)\n",
    "plt.xlabel('Hours of Hackathon'); plt.ylabel('Search Frequency')\n",
    "plt.savefig('search_histogram_local.png', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
